{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f178cd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f49b7dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\skill_gap_predictor\\venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\skill_gap_predictor\\venv\\lib\\site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\skill_gap_predictor\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\skill_gap_predictor\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\skill_gap_predictor\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\skill_gap_predictor\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Using cached spacy-3.8.2.tar.gz (1.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Ã— pip subprocess to install build dependencies did not run successfully.\n",
      "  â”‚ exit code: 1\n",
      "  â•°â”€> [63 lines of output]\n",
      "      Ignoring numpy: markers 'python_version < \"3.9\"' don't match your environment\n",
      "      Collecting setuptools\n",
      "        Using cached setuptools-78.1.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "      Collecting cython<3.0,>=0.25\n",
      "        Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)\n",
      "      Collecting cymem<2.1.0,>=2.0.2\n",
      "        Using cached cymem-2.0.11-cp313-cp313-win_amd64.whl.metadata (8.8 kB)\n",
      "      Collecting preshed<3.1.0,>=3.0.2\n",
      "        Using cached preshed-3.0.9.tar.gz (14 kB)\n",
      "        Installing build dependencies: started\n",
      "        Installing build dependencies: finished with status 'done'\n",
      "        Getting requirements to build wheel: started\n",
      "        Getting requirements to build wheel: finished with status 'done'\n",
      "        Preparing metadata (pyproject.toml): started\n",
      "        Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "      Collecting murmurhash<1.1.0,>=0.28.0\n",
      "        Using cached murmurhash-1.0.12-cp313-cp313-win_amd64.whl.metadata (2.2 kB)\n",
      "      Collecting thinc<8.4.0,>=8.3.0\n",
      "        Using cached thinc-8.3.6-cp313-cp313-win_amd64.whl.metadata (15 kB)\n",
      "      Collecting numpy<2.1.0,>=2.0.0\n",
      "        Using cached numpy-2.0.2.tar.gz (18.9 MB)\n",
      "        Installing build dependencies: started\n",
      "        Installing build dependencies: finished with status 'done'\n",
      "        Getting requirements to build wheel: started\n",
      "        Getting requirements to build wheel: finished with status 'done'\n",
      "        Installing backend dependencies: started\n",
      "        Installing backend dependencies: finished with status 'done'\n",
      "        Preparing metadata (pyproject.toml): started\n",
      "        Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "        error: subprocess-exited-with-error\n",
      "      \n",
      "        Ãƒâ€” Preparing metadata (pyproject.toml) did not run successfully.\n",
      "        Ã¢â€â€š exit code: 1\n",
      "        Ã¢â€¢Â°Ã¢â€â‚¬> [19 lines of output]\n",
      "            + C:\\Skill_Gap_Predictor\\venv\\Scripts\\python.exe C:\\Users\\palwe\\AppData\\Local\\Temp\\pip-install-zte35zus\\numpy_08c8cc17a0284eb48c0eca974891f4a7\\vendored-meson\\meson\\meson.py setup C:\\Users\\palwe\\AppData\\Local\\Temp\\pip-install-zte35zus\\numpy_08c8cc17a0284eb48c0eca974891f4a7 C:\\Users\\palwe\\AppData\\Local\\Temp\\pip-install-zte35zus\\numpy_08c8cc17a0284eb48c0eca974891f4a7\\.mesonpy-rf75v6a1 -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\palwe\\AppData\\Local\\Temp\\pip-install-zte35zus\\numpy_08c8cc17a0284eb48c0eca974891f4a7\\.mesonpy-rf75v6a1\\meson-python-native-file.ini\n",
      "            The Meson build system\n",
      "            Version: 1.4.99\n",
      "            Source dir: C:\\Users\\palwe\\AppData\\Local\\Temp\\pip-install-zte35zus\\numpy_08c8cc17a0284eb48c0eca974891f4a7\n",
      "            Build dir: C:\\Users\\palwe\\AppData\\Local\\Temp\\pip-install-zte35zus\\numpy_08c8cc17a0284eb48c0eca974891f4a7\\.mesonpy-rf75v6a1\n",
      "            Build type: native build\n",
      "            Project name: NumPy\n",
      "            Project version: 2.0.2\n",
      "            C compiler for the host machine: gcc (gcc 6.3.0 \"gcc (MinGW.org GCC-6.3.0-1) 6.3.0\")\n",
      "            C linker for the host machine: gcc ld.bfd 2.28\n",
      "            C++ compiler for the host machine: c++ (gcc 6.3.0 \"c++ (MinGW.org GCC-6.3.0-1) 6.3.0\")\n",
      "            C++ linker for the host machine: c++ ld.bfd 2.28\n",
      "            Cython compiler for the host machine: cython (cython 3.0.12)\n",
      "            Host machine cpu family: x86\n",
      "            Host machine cpu: x86\n",
      "      \n",
      "            ..\\meson.build:28:4: ERROR: Problem encountered: NumPy requires GCC >= 8.4\n",
      "      \n",
      "            A full log can be found at C:\\Users\\palwe\\AppData\\Local\\Temp\\pip-install-zte35zus\\numpy_08c8cc17a0284eb48c0eca974891f4a7\\.mesonpy-rf75v6a1\\meson-logs\\meson-log.txt\n",
      "            [end of output]\n",
      "      \n",
      "        note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "      error: metadata-generation-failed\n",
      "      \n",
      "      Ãƒâ€” Encountered error while generating package metadata.\n",
      "      Ã¢â€¢Â°Ã¢â€â‚¬> See above for output.\n",
      "      \n",
      "      note: This is an issue with the package mentioned above, not pip.\n",
      "      hint: See above for details.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "Ã— pip subprocess to install build dependencies did not run successfully.\n",
      "â”‚ exit code: 1\n",
      "â•°â”€> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\skill_gap_predictor\\venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp313-cp313-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\skill_gap_predictor\\venv\\lib\\site-packages (from scikit-learn) (2.2.4)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.15.2-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.6.1-cp313-cp313-win_amd64.whl (11.1 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached scipy-1.15.2-cp313-cp313-win_amd64.whl (41.0 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "c:\\Skill_Gap_Predictor\\venv\\Scripts\\python.exe: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install spacy\n",
    "!pip install tqdm\n",
    "!pip install scikit-learn\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82791ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Device check\n",
    "print(\"ğŸ“¦ Loading spaCy model...\")\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])  # Fast mode\n",
    "\n",
    "# ğŸ“‚ Load your dataset (change to .csv if needed)\n",
    "df = pd.read_csv(\"UpdatedResumeDataSet.csv\")  # ğŸ“ Make sure your file has \"Resume\" and \"Category\" columns\n",
    "df = df[[\"Category\", \"Resume\"]].dropna()\n",
    "\n",
    "# ğŸ§¹ Preprocess text\n",
    "def preprocess(text):\n",
    "    doc = nlp(text.lower())\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop and token.is_alpha])\n",
    "\n",
    "print(\"ğŸ§¼ Preprocessing resumes...\")\n",
    "tqdm.pandas()\n",
    "df[\"Cleaned_Resume\"] = df[\"Resume\"].progress_apply(preprocess)\n",
    "\n",
    "# ğŸ“Š TF-IDF Vectorization\n",
    "print(\"ğŸ”  Vectorizing...\")\n",
    "vectorizer = TfidfVectorizer(max_features=3000)\n",
    "X = vectorizer.fit_transform(df[\"Cleaned_Resume\"])\n",
    "y = df[\"Category\"]\n",
    "\n",
    "# ğŸ§  Train model\n",
    "print(\"ğŸ§  Training model...\")\n",
    "model = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "model.fit(X, y)\n",
    "\n",
    "# ğŸ’¾ Save model and vectorizer\n",
    "print(\"ğŸ’¾ Saving model and vectorizer...\")\n",
    "with open(\"resume_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "with open(\"resume_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "print(\"âœ… Training complete! Model saved as resume_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e206cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac46e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load RoBERTa model\n",
    "print(\"ğŸ“¥ Loading RoBERTa...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "roberta_model = RobertaModel.from_pretrained(\"roberta-base\").to(device)\n",
    "\n",
    "# Get embedding for a single text\n",
    "def get_roberta_embedding(text):\n",
    "    if not text.strip():\n",
    "        return np.zeros(768)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = roberta_model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "# Apply on all cleaned resumes\n",
    "print(\"ğŸ” Generating RoBERTa embeddings...\")\n",
    "roberta_embeddings = []\n",
    "\n",
    "for text in tqdm(df[\"Cleaned_Resume\"], desc=\"Embedding Resumes\"):\n",
    "    roberta_embeddings.append(get_roberta_embedding(text))\n",
    "    gc.collect()\n",
    "\n",
    "df[\"RoBERTa_Embedding\"] = roberta_embeddings\n",
    "\n",
    "# Save to pickle file for later use\n",
    "df.to_pickle(\"resume_roberta_embeddings.pkl\")\n",
    "print(\"âœ… RoBERTa embeddings saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3070342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing code\n",
    "import PyPDF2\n",
    "import docx\n",
    "import spacy\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from thefuzz import fuzz\n",
    "\n",
    "# ========== Load Models ==========\n",
    "print(\"ğŸ”§ Loading models...\")\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "roberta_model = RobertaModel.from_pretrained(\"roberta-base\").to(device)\n",
    "\n",
    "with open(\"resume_model.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "with open(\"resume_vectorizer.pkl\", \"rb\") as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "\n",
    "# ========== Utility Functions ==========\n",
    "def preprocess(text):\n",
    "    doc = nlp(text.lower())\n",
    "    return \" \".join([token.lemma_ for token in doc if token.is_alpha and not token.is_stop])\n",
    "\n",
    "def get_roberta_embedding(text):\n",
    "    if not text.strip():\n",
    "        return np.zeros(768)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = roberta_model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "def extract_text_from_file(filepath):\n",
    "    if filepath.endswith(\".pdf\"):\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            return \"\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "    elif filepath.endswith(\".docx\"):\n",
    "        doc = docx.Document(filepath)\n",
    "        return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def match_skills_fuzzy(resume_text, required_skills, threshold=80):\n",
    "    matched = []\n",
    "    resume_lines = resume_text.lower().splitlines()\n",
    "    for skill in required_skills:\n",
    "        for line in resume_lines:\n",
    "            if fuzz.partial_ratio(skill.lower(), line) >= threshold:\n",
    "                matched.append(skill)\n",
    "                break\n",
    "    return list(set(matched))\n",
    "\n",
    "def recommend_courses(missing_skills):\n",
    "    course_dict = {\n",
    "        \"python\": \"https://www.udemy.com/course/python-for-beginners/\",\n",
    "        \"data science\": \"https://www.udemy.com/course/data-science-python/\",\n",
    "        \"machine learning\": \"https://www.udemy.com/course/machinelearning/\",\n",
    "        \"sql\": \"https://www.udemy.com/course/sql-for-data-analysis/\",\n",
    "        \"deep learning\": \"https://www.udemy.com/course/deeplearning/\",\n",
    "        \"statistics\": \"https://www.udemy.com/course/statistics-for-data-science/\"\n",
    "    }\n",
    "    return {skill: course_dict.get(skill.lower(), \"ğŸ” Course not found. Search manually.\") for skill in missing_skills}\n",
    "\n",
    "# ========== Input Section ==========\n",
    "resume_path = input(\"ğŸ“„ Enter full path to resume (PDF/DOCX): \").strip()\n",
    "job_description = input(\"ğŸ“ Enter job description: \").strip()\n",
    "required_skills_input = input(\"ğŸ’¡ Enter required skills (comma separated): \").strip()\n",
    "required_skills = [skill.strip() for skill in required_skills_input.split(\",\") if skill.strip()]\n",
    "\n",
    "# ========== Processing ==========\n",
    "print(\"\\nâ³ Extracting and analyzing resume...\")\n",
    "resume_text = extract_text_from_file(resume_path)\n",
    "\n",
    "if not resume_text.strip():\n",
    "    print(\"âŒ Could not extract text from resume.\")\n",
    "else:\n",
    "    # Preprocess\n",
    "    cleaned_resume = preprocess(resume_text)\n",
    "    cleaned_jd = preprocess(job_description)\n",
    "\n",
    "    # Predict Job Category\n",
    "    tfidf_vec = vectorizer.transform([cleaned_resume])\n",
    "    predicted_category = model.predict(tfidf_vec)[0]\n",
    "\n",
    "    # Semantic Similarity\n",
    "    resume_embed = get_roberta_embedding(cleaned_resume)\n",
    "    jd_embed = get_roberta_embedding(cleaned_jd)\n",
    "    similarity = cosine_similarity([resume_embed], [jd_embed])[0][0]\n",
    "\n",
    "    # Skill Matching with Fuzzy Logic\n",
    "    matched_skills = match_skills_fuzzy(resume_text, required_skills, threshold=80)\n",
    "    missing_skills = [skill for skill in required_skills if skill not in matched_skills]\n",
    "\n",
    "    # Course Recommendations\n",
    "    recommended_courses = recommend_courses(missing_skills)\n",
    "\n",
    "    # Fit Score Calculation\n",
    "    skill_match_score = (len(matched_skills) / len(required_skills)) * 100 if required_skills else 0\n",
    "    semantic_score = round(similarity * 100, 2)\n",
    "    final_fit_score = round((semantic_score * 0.6) + (skill_match_score * 0.4), 2)\n",
    "\n",
    "    # ========== Output ==========\n",
    "    print(\"\\nâœ… Final Results:\")\n",
    "    print(f\"ğŸ“Œ Predicted Job Category: {predicted_category}\")\n",
    "    print(f\"ğŸ”— Semantic Similarity Score (Resume â†” JD): {semantic_score}%\")\n",
    "    print(f\"ğŸ¯ Skill Match Score: {round(skill_match_score, 2)}%\")\n",
    "    print(f\"ğŸ§  Final Resumeâ€“JD Fit Score: {final_fit_score}%\")\n",
    "\n",
    "    print(f\"âœ… Matched Skills: {matched_skills if matched_skills else 'None'}\")\n",
    "    print(f\"âŒ Missing Skills: {missing_skills if missing_skills else 'None'}\")\n",
    "\n",
    "    if missing_skills:\n",
    "        print(\"\\nğŸ“š Recommended Courses for Missing Skills:\")\n",
    "        for skill, course in recommended_courses.items():\n",
    "            print(f\"  - {skill}: {course}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
